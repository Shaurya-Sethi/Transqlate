# ======================================================
#  Shared model / tokenizer
# ======================================================
model:
  vocab_size:        32000
  d_model:           1344          # 350M parameters
  n_heads:           21            # Must divide d_model
  n_layers:          12            # ~350M params
  num_relations:     5
  dropout:           0.1
  max_len:           2048          # Max context length (for eval/inference)
  use_pointer_generator: true

tokenizer:
  pad_token_id: 18
  special_tokens:
    SCHEMA_START: "<SCHEMA>"
    SCHEMA_END:   "</SCHEMA>"
    PK_START:     "<PK>"
    PK_END:       "</PK>"
    FK_START:     "<FK>"
    FK_END:       "</FK>"
    NL_START:     "<NL>"
    NL_END:       "</NL>"
    COT_START:    "<COT>"
    COT_END:      "</COT>"
    SQL_START:    "<SQL>"
    SQL_END:      "</SQL>"
    EXT_START:    "<EXT>"
    EXT_END:      "</EXT>"

# ======================================================
#  Phase-specific hyper-parameters
# ======================================================
pretraining:
  train_file:                 "datasets/raw_sql/pretraining corpus/splits/wrapped_tokenized_corpus_train.txt"
  eval_file:                  "datasets/raw_sql/pretraining corpus/splits/wrapped_tokenized_corpus_val.txt"
  epochs:                     3
  max_len:                    512         # Use shorter sequences for pretraining (memory/cost saver)
  # ------------------- GPU-mem optimized batch -----------------------
  micro_batch_size:           8            # Lowered for T4/V100 (from 16)
  max_batch_size:             16
  gradient_accumulation:      8            # To keep effective batch = 64
  # ------------------- optimization ----------------------------------
  learning_rate:              2.0e-4
  scheduler:                  cosine
  warmup_steps:               3000
  max_steps:                  46875
  weight_decay:               0.01
  max_grad_norm:              1.0
  # ------------------- runtime ---------------------------------------
  mixed_precision:            true         # Use fp16 (not bf16)
  use_8bit_optimizer:         true
  bf16:                       false        # Only on A100; T4/V100 use fp16
  gradient_checkpointing:     true
  save_steps:                 2000
  num_workers:                4
  early_stopping_patience:    null

sft:
  train_file:                 "datasets/paired_nl_sql/splits/tokenized_sft_filtered_train.txt"
  eval_file:                  "datasets/paired_nl_sql/splits/tokenized_sft_filtered_val.txt"
  epochs:                     3
  # ------------------- GPU-mem optimized batch -----------------------
  micro_batch_size:           1            # 2048-token context is heavy; batch 1 is safest
  max_batch_size:             16
  gradient_accumulation:      32           # To keep effective batch = 32
  # ------------------- optimization ----------------------------------
  learning_rate:              5.0e-5
  scheduler:                  cosine
  warmup_steps:               3000
  max_steps:                  206250
  weight_decay:               0.01
  max_grad_norm:              1.0
  # ------------------- SFT length constraints ------------------------
  phase_max_len:              1664
  max_sql_len:                320
  # ------------------- runtime ---------------------------------------
  mixed_precision:            true
  bf16:                       false
  use_8bit_optimizer:         true
  gradient_checkpointing:     true
  save_steps:                 1000
  num_workers:                4
  early_stopping_patience:    3

# ======================================================
#  Misc paths / outputs
# ======================================================
paths:
  sp_model:   "models/nl2sql_tok.model"
  output_dir: "outputs"

# ======================================================
#  Logging configuration
# ======================================================
logging:
  tensorboard_log_dir: "runs"
  log_every_n_steps: 10
  log_grad_norm: true
  log_grad_histogram: false
  log_memory: true