# ======================================================
#  Shared model / tokenizer
# ======================================================
model:
  vocab_size:        32000
  d_model:           1344       # Using user's calculated block size
  n_heads:           21      # Must be a divisor of d_model
  n_layers:          6        # Kept at 12 for good depth
  num_relations:     5
  dropout:           0.1
  max_len:           2048       # Maximum sequence length
  use_pointer_generator: true   # Use pointer-generator for copying schema tokens

tokenizer:
  pad_token_id: 18             # Keep our current pad_token_id
  special_tokens:
    SCHEMA_START: "<SCHEMA>"
    SCHEMA_END:   "</SCHEMA>"
    PK_START:     "<PK>"
    PK_END:       "</PK>"
    FK_START:     "<FK>"
    FK_END:       "</FK>"
    NL_START:     "<NL>"
    NL_END:       "</NL>"
    COT_START:    "<COT>"
    COT_END:      "</COT>"
    SQL_START:    "<SQL>"
    SQL_END:      "</SQL>"
    EXT_START:    "<EXT>"
    EXT_END:      "</EXT>"

# ======================================================
#  Phase-specific hyper-parameters
# ======================================================
pretraining:
  # ---------- data (decoder-only SQL corpus) ------------------------- #
  train_file:                 "datasets/raw_sql/pretraining corpus/splits/wrapped_tokenized_corpus_train.txt"
  eval_file:                  "datasets/raw_sql/pretraining corpus/splits/wrapped_tokenized_corpus_val.txt"
  epochs:                     3
  # ---------- micro-batch & accumulation ---------------------------- #
  micro_batch_size:           16        # fits comfortably in 40 GB
  max_batch_size:            32         # Maximum batch size for validation
  gradient_accumulation:      4         # effective batch = 64
  # ---------- optimisation ------------------------------------------ #
  learning_rate:              3.0e-4
  scheduler:                  cosine
  warmup_steps:               3000      # ≈6% of total steps
  max_steps:                  46875     # 3 epochs × 1M / 64
  weight_decay:               0.10
  max_grad_norm:              1.0
  # ---------- runtime ----------------------------------------------- #
  mixed_precision:            true      # fp16
  use_8bit_optimizer:         true
  bf16:                       false
  gradient_checkpointing:     true
  save_steps:                 2000
  num_workers:                4
  early_stopping_patience:    null      # disable early stop

sft:
  # ---------- data (question+schema+CoT+SQL) ------------------------ #
  train_file:                 "datasets/paired_nl_sql/splits/tokenized_sft_filtered_train.txt"
  eval_file:                  "datasets/paired_nl_sql/splits/tokenized_sft_filtered_val.txt"
  epochs:                     2
  # ---------- micro-batch & accumulation ---------------------------- #
  micro_batch_size:           2         # long 2048-token enc/dec pairs
  max_batch_size:            32         # Maximum batch size for validation
  gradient_accumulation:      16        # effective batch = 32
  # ---------- optimisation ------------------------------------------ #
  learning_rate:              5.0e-5
  scheduler:                  cosine
  warmup_steps:               3000      # ≈2% of total steps
  max_steps:                  137500    # 2 epochs × 2.2M / 32
  weight_decay:               0.01
  max_grad_norm:              1.0
  # ---------- runtime ----------------------------------------------- #
  mixed_precision:            true
  use_8bit_optimizer:         true
  bf16:                       false
  gradient_checkpointing:     true
  save_steps:                 1000
  num_workers:                4
  early_stopping_patience:    3

# ======================================================
#  Misc paths / outputs
# ======================================================
paths:
  sp_model:   "models/nl2sql_tok.model"  # Keep current model path
  output_dir: "outputs" 