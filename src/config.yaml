# ======================================================
#  Shared model / tokenizer - L4 BASELINE
# ======================================================
model:
  vocab_size:        32000
  d_model:           768           # ↓ from 1344
  n_heads:           12            # must divide d_model
  n_layers:          8             # ↓ from 12
  num_relations:     5
  dropout:           0.1
  max_len:           2048
  use_pointer_generator: true

tokenizer:
  pad_token_id: 18
  special_tokens:
    SCHEMA_START: "<SCHEMA>"
    SCHEMA_END:   "</SCHEMA>"
    PK_START:     "<PK>"
    PK_END:       "</PK>"
    FK_START:     "<FK>"
    FK_END:       "</FK>"
    NL_START:     "<NL>"
    NL_END:       "</NL>"
    COT_START:    "<COT>"
    COT_END:      "</COT>"
    SQL_START:    "<SQL>"
    SQL_END:      "</SQL>"
    EXT_START:    "<EXT>"
    EXT_END:      "</EXT>"

# ======================================================
#  Phase-specific hyper-parameters (L4-optimised)
# ======================================================
pretrain:
  # ---------- data ----------------------------------- #
  train_file:  "src/datasets/raw_sql/pretraining corpus/splits/wrapped_tokenized_corpus_train.txt"
  eval_file:   "src/datasets/raw_sql/pretraining corpus/splits/wrapped_tokenized_corpus_val.txt"
  epochs:      4 
  max_len:     512

  # ---------- batch / accumulation ------------------- #
  micro_batch_size:      4       # fits with new model
  gradient_accumulation: 8       # eff-batch 32
  max_batch_size:        16      # validation

  # ---------- optimisation --------------------------- #
  learning_rate:   1.0e-4
  scheduler:       cosine
  warmup_steps:    2500
  max_steps:       46875
  weight_decay:    0.01
  max_grad_norm:   0.3           # reduced from 0.5 -> more aggressive clipping

  # ---------- runtime -------------------------------- #
  mixed_precision:        true     # fp16/ bf16 autocast
  bf16:                   true     # L4 supports bf16; gives more headroom
  use_8bit_optimizer:     true
  gradient_checkpointing: true
  save_steps:             2000        # Save a checkpoint after 2000 steps
  num_workers:            4
  early_stopping_patience: null

sft:
  # ---------- data ----------------------------------- #
  train_file:  "src/datasets/paired_nl_sql/splits/tokenized_sft_filtered_train.txt"
  eval_file:   "src/datasets/paired_nl_sql/splits/tokenized_sft_filtered_val.txt"
  epochs:      2

  # ---------- batch / accumulation ------------------- #
  micro_batch_size:      1        # fits 1664-token context
  gradient_accumulation: 32       # eff-batch 32
  max_batch_size:        8

  # ---------- optimisation --------------------------- #
  learning_rate:   3.0e-5
  scheduler:       cosine
  warmup_steps:    100
  max_steps:       206250
  weight_decay:    0.01
  max_grad_norm:   1.0

  # ---------- length constraints --------------------- #
  phase_max_len:   1664
  max_sql_len:     320

  # ---------- runtime -------------------------------- #
  mixed_precision:        true
  bf16:                   true     # enable bf16 for extra VRAM savings
  use_8bit_optimizer:     true
  gradient_checkpointing: true
  save_steps:             20000
  num_workers:            4
  early_stopping_patience: 3

# ======================================================
#  Misc paths / outputs
# ======================================================
paths:
  sp_model:   "models/nl2sql_tok.model"
  output_dir: "outputs"

# ======================================================
#  Logging configuration
# ======================================================
logging:
  tensorboard_log_dir: "runs"
  log_every_n_steps:   10
  log_grad_norm:       true
  log_grad_histogram:  false
  log_memory:          true
